# -*- coding: utf-8 -*-
"""FDA_T1_CREDITRISK4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17gK2SuTFZ3xmBVMvnKkp_YUWERqG3jFK

#Librerias
"""
import os
import sys
import glob

import numpy as np
import pandas as pd

import matplotlib.pylab as plt
from matplotlib_venn import venn2
import seaborn as sns

from tqdm import tqdm
from itertools import cycle

from sklearn import metrics
from sklearn import model_selection
from sklearn import preprocessing
from sklearn import linear_model
from sklearn import feature_selection

import lightgbm as lgb
import xgboost as xgb
import catboost as cat

import optbinning

import io

from scipy import stats

from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score

#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency

"""# Funciones utilizadas"""

# function to create dummy variables
def dummy_creation(df, columns_list):
    df_dummies = []
    for col in columns_list:
        df_dummies.append(pd.get_dummies(df[col], prefix = col, prefix_sep = ':'))
    df_dummies = pd.concat(df_dummies, axis = 1)
    df = pd.concat([df, df_dummies], axis = 1)
    return df


# function to calculate WoE and IV of categorical features
# The function takes 3 arguments: a dataframe (X_train_prepr), a string (column name), and a dataframe (y_train_prepr).
def woe_discrete(df, cat_variabe_name, y_df):
    df = pd.concat([df[cat_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    df = df.sort_values(['WoE'])
    df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

# We set the default style of the graphs to the seaborn style.
sns.set()
# Below we define a function for plotting WoE across categories that takes 2 arguments: a dataframe and a number.
def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):
    x = np.array(df_WoE.iloc[:, 0].apply(str))
    y = df_WoE['WoE']
    plt.figure(figsize=(18, 6))
    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')
    plt.xlabel(df_WoE.columns[0])
    plt.ylabel('Weight of Evidence')
    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))
    plt.xticks(rotation = rotation_of_x_axis_labels)

def woe_ordered_continuous(df, continuous_variabe_name, y_df):
    df = pd.concat([df[continuous_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    #df = df.sort_values(['WoE'])
    #df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

"""# Lectura base de datos"""

data = pd.read_csv('cscore\files\credit_risk_dataset.csv', sep=',')

print(len(data))
data.head()

"""# Pre-procesamiento"""

df = data.dropna(subset=['person_emp_length'])
print(len(df))
df.head()

df['person_emp_length'].plot(kind='hist', bins=50)
plt.xlabel('Valores')
plt.ylabel('Frecuencia')
plt.title('Histograma de la columna')
plt.show()

"""por el momento la solucion para los valores faltantes de person_emp_lenght es borrar los registros que tengan ese valor en nulo.
pero se debe analizar la significancia de esta variable para nuestro modelo ya que la primera opcion seria eliminar la columna en caso de que en definitiva no aporte algo al modelo, como segunda opcion la eliminacion de registros
"""

print(df.isna().sum().sum())

"""todavia hay 3048 nulos, por lo cual no es viable eliminar registros, representan casi el 10% de los datos

ahora para la variable, "loan_int_rate"

Ya que esos datos nulos representan un 10% de los datos, se usan metodos para remuestrear o imputar esos valores nulos

Inicialmente se vio la viabilidad de realizar la imputacion por medio de un modelo KNN, pero no es muy viable por la precision que podria tener en el contexto
"""

tasa_interes1 = df['loan_int_rate']
stats.probplot(tasa_interes1, dist="norm", plot=plt)
plt.title('QQ-Plot de la columna')
plt.show()

"""A la final la imputacion se realiza generando numeros aleatorios que siguen una distribucion normal con media 11 y desviacion estandar 3.24"""

df2 = df.copy()
filas_nulas = df2['loan_int_rate'].isnull()
valores_aleatorios = np.random.normal(11, 3.24, filas_nulas.sum())
df2['loan_int_rate'].fillna(pd.Series(valores_aleatorios, index=df.index[filas_nulas]), inplace=True)

print(len(df2))

df2['loan_int_rate'].plot(kind='hist', bins=50)
plt.xlabel('Valores')
plt.ylabel('Frecuencia')
plt.title('Histograma de la columna')
plt.show()

"""Eliminacion de datos atipicos segun el contexto"""

#se toma las edades menores a 100
mascara_age = df2['person_age'] <= 100
df3 = df2[mascara_age]

#crear mascara que tome del dataframe solo las personas que su edad sea mayor que el tiempo que haya sido empleado
mascara = df3['person_age'] > df3['person_emp_length']
df4 = df3[mascara]
df4.head()

# calculate pair-wise correlations between them
datanum = df4.select_dtypes(include = 'number').copy()
corrmat = datanum.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corrmat, vmax =.8, square = True, annot = True)

df4.describe()

"""Para esta variable "loan_int_rate" notamos varios supuestos valores atipicos, veamoslo graficamente:"""

tasa_interes = df4['loan_int_rate']

plt.boxplot([tasa_interes])
plt.ylabel('Valores')
plt.title('Gráfico de caja de la Serie')
plt.show()

Q1 = tasa_interes.quantile(0.25)
Q3 = tasa_interes.quantile(0.75)
IQR = Q3-Q1
lim_sup = (Q3+1.5*IQR)
lim_inf = (Q1-1.5*IQR)
# Imprime los valores mínimo y máximo de la caja (sin contar los valores atípicos)
print("Valor mínimo de la caja en el gráfico de caja:", lim_inf)
print("Valor máximo de la caja en el gráfico de caja:", lim_sup)

tasa_tipicos = tasa_interes[(tasa_interes >= 0.455) & (tasa_interes <= 21.825)]
mascara = (tasa_interes > 22) | (tasa_interes < 0.4)
mascara1 = (tasa_interes >= 0.4) & (tasa_interes <= 22)

# Sumar los valores que cumplen con la máscara
suma_valores_deseados = tasa_interes[mascara1]
print(len(suma_valores_deseados))
print(len(tasa_interes))

"""Se pueden descartar 7 valores como atipicos y finalmente nuestro dataframe estaria definido como df4"""

df4=df4[mascara1]

"""# Preparacion de los datos"""

# revisar cuantas observaciones hay en cada clase de la variable objetivo
df4['loan_status'].value_counts(normalize = True)

"""Se particionan los datos en la proporcion (80/20) en: datos de entrenamiento y datos de testeo.\
**datos de entrenamiento:** \
**datos de testeo:**
"""

# split data into 80/20 while keeping the distribution of bad loans in test set same as that in the pre-split dataset
X = df4.drop('loan_status', axis = 1)
y = df4['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,
                                                    random_state = 42, stratify = y)

# hard copy the X datasets to avoid Pandas' SetttingWithCopyWarning when we play around with this data later on.
# this is currently an open issue between Pandas and Scikit-Learn teams
X_train, X_test = X_train.copy(), X_test.copy()

#Se eliminan las columnas debido al IV
X_train2 = X_train.drop(['person_age','cb_person_cred_hist_length'], axis = 1)
cols=[col for col in X_train2]

X_test2 = X_test.drop(['person_age','cb_person_cred_hist_length'], axis = 1)

"""Se elimino las columnas ('person_age','cb_person_cred_hist_length') debido al IV como veremos mas adelante"""

# first divide training data into categorical and numerical subsets
X_train_cat = X_train.select_dtypes(include = 'object').copy()
X_train_num = X_train.select_dtypes(include = 'number').copy()
X_test_num = X_test.select_dtypes(include = 'number').copy()

"""identificamos el tipo de variables que tenemos: 4 variables categoricas y 7 variables numericas

## Significancia para variables categoricas

Significancia para las variables categoricas segun prueba chi2
"""

# define an empty dictionary to store chi-squared test results
chi2_check = {}

# loop over each column in the training set to calculate chi-statistic with the target variable
for column in X_train_cat:
    chi, p, dof, ex = chi2_contingency(pd.crosstab(y_train, X_train_cat[column]))
    chi2_check.setdefault('Feature',[]).append(column)
    chi2_check.setdefault('p-value',[]).append(round(p, 10))

# convert the dictionary to a DF
chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)

chi2_result

"""A nuestro criterio ninguna variable categorica tiene alguna jerarquia por lo cual convertimos las variables categoricas haciendo uso de la funcion dummy variables"""

# apply to our final four categorical variables
X_train = dummy_creation(X_train, ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])
X_test = dummy_creation(X_test, ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])
# reindex the dummied test set variables to make sure all the feature columns in the training set are also available in the test set
X_test = X_test.reindex(labels=X_train.columns, axis=1, fill_value=0)

X_test

# Create copies of the 4 training sets to be preprocessed using WoE

X_train_prepr = X_train.copy()
y_train_prepr = y_train.copy()
X_test_prepr = X_test.copy()
y_test_prepr = y_test.copy()

"""IV, OptimalBinning & WoE : *person_home_ownership*"""

woe_discrete(X_train_prepr,'person_home_ownership',y_train_prepr)

col = 'person_home_ownership'
optb = optbinning.OptimalBinning(dtype='categorical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""IV, OptimalBinning & WoE : *loan_intent*"""

woe_discrete(X_train_prepr,'loan_intent',y_train_prepr)

col = 'loan_intent'
optb = optbinning.OptimalBinning(dtype='categorical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""IV, OptimalBinning & WoE : *loan_grade*"""

woe_discrete(X_train_prepr,'loan_grade',y_train_prepr)

col = 'loan_grade'
optb = optbinning.OptimalBinning(dtype='categorical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""IV, OptimalBinning & WoE : *cb_person_default_on_file*"""

woe_discrete(X_train_prepr,'cb_person_default_on_file',y_train_prepr)

col = 'cb_person_default_on_file'
optb = optbinning.OptimalBinning(dtype='categorical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""## Significancia variables numericas

Significancia para las variables numericas segun ANOVA
"""

# since f_class_if does not accept missing values, we will do a very crude imputation of missing values
X_train_num.fillna(X_train_num.mean(), inplace = True)

# Calculate F Statistic and corresponding p values
F_statistic, p_values = f_classif(X_train_num, y_train)

# convert to a DF
ANOVA_F_table = pd.DataFrame(data = {'Numerical_Feature': X_train_num.columns.values,
					'F-Score': F_statistic, 'p values': p_values.round(decimals=10)})
ANOVA_F_table.sort_values(by = ['F-Score'], ascending = False, ignore_index = True, inplace = True)

ANOVA_F_table

# calculate pair-wise correlations between them
corrmat = X_train_num.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corrmat, vmax =.8, square = True, annot = True)

correlation_matrix = X_train_num.corr()
print(correlation_matrix)

"""WoE & OptimalBinning: *loan_percent_income*"""

# REVISAR
# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['loan_percent_income_factor'] = pd.cut(X_train_prepr['loan_percent_income'], 50)

woe_ordered_continuous(X_train_prepr, 'loan_percent_income', y_train_prepr)

col = 'loan_percent_income'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

plot_by_woe(woe_ordered_continuous(X_train_prepr, 'loan_percent_income', y_train_prepr))

"""WoE & OptimalBinning: *loan_int_rate*"""

woe_ordered_continuous(X_train_prepr, 'loan_int_rate', y_train_prepr)

col = 'loan_int_rate'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""WoE & OptimalBinning: *person_income*"""

woe_ordered_continuous(X_train_prepr, 'person_income', y_train_prepr)

col = 'person_income'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""WoE & OptimalBinning: *loan_amnt*"""

woe_ordered_continuous(X_train_prepr, 'loan_amnt', y_train_prepr)

col = 'loan_amnt'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""WoE & OptimalBinning: *person_emp_length*"""

woe_ordered_continuous(X_train_prepr, 'person_emp_length', y_train_prepr)

col = 'person_emp_length'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""WoE & OptimalBinning: *person_age*"""

woe_ordered_continuous(X_train_prepr, 'person_age', y_train_prepr)

col = 'person_age'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""WoE & OptimalBinning: *cb_person_cred_hist_length*"""

woe_ordered_continuous(X_train_prepr, 'cb_person_cred_hist_length', y_train_prepr)

col = 'cb_person_cred_hist_length'
optb = optbinning.OptimalBinning(dtype='numerical')
optb.fit(X_train_prepr[col], y_train_prepr)
binning_table = optb.binning_table
print(binning_table.build())

"""La decision se toma bajo este ultimo criterio donde, se elimina person_age y cb_person_cred_hist_length

## Seleccion de caracteristicas
"""

X_train = X_train.drop(['person_age','cb_person_cred_hist_length'], axis = 1)

"""# Modelo de regresion logistica

Inicializar instancia de la clase BinningProcess
"""

binning_process = optbinning.BinningProcess(
    variable_names=cols,
    #selection_criteria=selection_criteria,
    categorical_variables=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
)

"""Estimador para el modelo de regresión logística, el cual permite hasta 1000 iteraciones. tambien el estimador se define el class_Weight para atacar el problema del desbalanceo de clases."""

estimator = linear_model.LogisticRegression(max_iter=1000, class_weight = 'balanced')

"""La función scorecard configura un objeto Scorecard que se utilizará para calificar a los solicitantes de crédito, recibe como entrada la probabilidad de incumplimiento crediticio y devuelve el puntaje ya escalado entre 300 y 800."""

# scorecard
scorecard = optbinning.Scorecard(
    binning_process=binning_process,
    estimator=estimator,
    scaling_method="min_max",
    scaling_method_params={"min": 300, "max": 800},
    #scaling_method = "pdo_odds",
    #scaling_method_params = {"pdo": 20, "odds": 50, "scorecard_points": 100},
    #intercept_based=True,
    #reverse_scorecard=True
)

y_train

"""Ajuste del modelo"""

# model fitting
scorecard.fit(X_train, y_train)

# scorecard table
scorecard_df = scorecard.table(style="detailed")
scorecard_df.query("Variable == 'loan_grade'")

X_train2['score'] = scorecard.score(X_train)
X_test2['score'] = scorecard.score(X_test)



X_test2

score = X_test2['score']

mask = y_test == 0

fig, ax = plt.subplots(figsize=(20,10))
plt.hist(score[mask], label="non-default", color="b", alpha=0.35)
plt.hist(score[~mask], label="default", color="r", alpha=0.35)
plt.xlabel("score")
plt.legend()
plt.show()

import pickle

with open('modelo_scorecard.pkl', 'wb') as archivo_pkl:
    pickle.dump(scorecard, archivo_pkl)

obsc = cols
obsdf = pd.DataFrame(columns = obsc)


obsdf=obsdf._append({'person_income':60000,
                    'person_home_ownership':'MORTGAGE',
                    'person_emp_length':17.0,
                    'loan_intent':'VENTURE',
                    'loan_grade':'A',
                    'loan_amnt':7200,
                    'loan_int_rate':16.29,
                    'loan_percent_income':0.7,
                    'cb_person_default_on_file':'N'},ignore_index=True)

score = scorecard.score(obsdf)

score

X_test2

X_test2['score'] = scorecard.predict_proba(X_test)

X_test2